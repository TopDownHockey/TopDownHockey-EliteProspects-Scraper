{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-worry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4  import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "from requests import ConnectionError, ReadTimeout, ConnectTimeout, HTTPError, Timeout\n",
    "\n",
    "class Patrick(Exception): pass\n",
    "\n",
    "def tableDataText(table):\n",
    "    rows = []\n",
    "    trs = table.find_all('tr')\n",
    "\n",
    "    headerow = [td.get_text(strip=True) for td in trs[0].find_all('th')] # header row\n",
    "    if headerow: # if there is a header row include first\n",
    "        rows.append(headerow)\n",
    "        trs = trs[1:]\n",
    "    for tr in trs: # for every table row\n",
    "        rows.append([td.get_text(strip=True) for td in tr.find_all('td')]) # data row\n",
    "        \n",
    "    df_rows = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "    \n",
    "    return df_rows\n",
    "\n",
    "def getskaters(league, year):  \n",
    "    \"\"\"\n",
    "    Get all players for specific year and league; returns dataframe\n",
    "    League input in format '2018-19'\n",
    "    \"\"\"\n",
    "   \n",
    "    url = 'https://www.eliteprospects.com/league/' + league + '/stats/' + year + '?page='\n",
    "    # print('Collects data from ' + 'https://www.eliteprospects.com/league/' + league + '/stats/' + year)\n",
    "    \n",
    "    print(\"Beginning scrape of \" + league + \" skater data from \" + year + \".\")\n",
    "    \n",
    "    # Return list with all plyers for season in link     \n",
    "    players = []\n",
    "    \n",
    "    page = (requests.get(url+str(1), timeout = 500))\n",
    "    first_page_string = str(page)\n",
    "    \n",
    "    while first_page_string == '<Response [403]>':\n",
    "        print(\"Just got a 403 Error before entering the page. Time to Sleep, then re-obtain the link.\")\n",
    "        time.sleep(100)\n",
    "        page = (requests.get(url+str(1), timeout = 500))\n",
    "        first_page_string = str(page)\n",
    "        print(\"Changed the string before entering the page. Let's try again\")\n",
    "    \n",
    "    if (str(first_page_string) == '<Response [404]>'):\n",
    "        print(\"ERROR: \" + str(first_page_string) + \" on league: \" + league + \" in year: \" + year + \". Data doesn't exist for this league in this year.\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(1,99):\n",
    "            page = requests.get(url+str(i), timeout = 500) \n",
    "            page_string = str(page)\n",
    "            \n",
    "            while page_string == '<Response [403]>':\n",
    "                print(\"Just got a 403 Error within the page. Time to Sleep, then re-obtain the link.\")\n",
    "                time.sleep(100)\n",
    "                page = requests.get(url+str(i), timeout = 500) \n",
    "                page_string = str(page)\n",
    "                print(\"Changed the string within the page. Let's try again\")\n",
    "                \n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "            # Get data for players table\n",
    "            player_table = soup.find( \"table\", {\"class\":\"table table-striped table-sortable player-stats highlight-stats season\"})\n",
    "            \n",
    "            try:\n",
    "                df_players = tableDataText(player_table)\n",
    "                \n",
    "            except AttributeError:\n",
    "                print(\"BREAK: TABLE NONE ERROR: \" + str(requests.get(url+str(i)), timeout = 500) + \" On League: \" + league + \" In Year: \" + year)\n",
    "                break\n",
    "                \n",
    "            if len(df_players)>0:\n",
    "\n",
    "                if df_players['#'].count()>0:\n",
    "                    # Remove empty rows\n",
    "                    df_players = df_players[df_players['#']!=''].reset_index(drop=True)\n",
    "\n",
    "                    # Extract href links in table\n",
    "                    href_row = []\n",
    "                    for link in player_table.find_all('a'):\n",
    "                        href_row.append(link.attrs['href'])\n",
    "\n",
    "                    # Create data frame, rename and only keep links to players\n",
    "                    df_links = pd.DataFrame(href_row)  \n",
    "                    df_links.rename(columns={ df_links.columns[0]:\"link\"}, inplace=True)\n",
    "                    df_links= df_links[df_links['link'].str.contains(\"/player/\")].reset_index(drop=True)    \n",
    "\n",
    "                    # Add links to players\n",
    "                    df_players['link']=df_links['link'] \n",
    "\n",
    "                    players.append(df_players)\n",
    "\n",
    "                    # Wait 3 seconds before going to next\n",
    "                    #time.sleep(1)\n",
    "                    #print(\"Scraped page \" + str(i))\n",
    "                    \n",
    "            else:\n",
    "                #print(\"Scraped final page of: \" + league + \" In Year: \" + year)\n",
    "                break\n",
    "\n",
    "    \n",
    "        if len(players)!=0:\n",
    "            df_players = pd.concat(players).reset_index()\n",
    "\n",
    "            df_players.columns = map(str.lower, df_players.columns)\n",
    "\n",
    "            # Clean up dataset\n",
    "            df_players['season'] = year\n",
    "            df_players['league'] = league\n",
    "\n",
    "            df_players = df_players.drop(['index','#'], axis=1).reset_index(drop=True)\n",
    "\n",
    "            df_players['playername'] = df_players['player'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "            df_players['position'] = df_players['player'].str.extract('.*\\((.*)\\).*')\n",
    "            df_players['position'] = np.where(pd.isna(df_players['position']), \"F\", df_players['position'])\n",
    "\n",
    "            df_players['fw_def'] = df_players['position'].str.contains('LW|RW|C|F')\n",
    "            df_players.loc[df_players['position'].str.contains('LW|RW|C'), 'fw_def'] = 'FW'\n",
    "            df_players.loc[df_players['position'].str.contains('D'), 'fw_def'] = 'DEF'\n",
    "\n",
    "            # Adjust columns; transform data\n",
    "            team = df_players['team'].str.split(\"â€œ\", n=1, expand=True)\n",
    "            df_players['team'] = team[0]\n",
    "\n",
    "            # drop player-column\n",
    "            df_players = df_players.drop(columns = ['fw_def'], axis=1)\n",
    "            print(\"Successfully scraped all \" + league + \" skater data from \" + year + \".\")\n",
    "\n",
    "            return df_players\n",
    "        \n",
    "        else: print(\"LENGTH 0 ERROR: \" + str(requests.get(url+str(1)), timeout = 500) + \" On League: \" + league + \" In Year: \" + year)\n",
    "            \n",
    "def getgoalies(league, year):\n",
    "\n",
    "    url = 'https://www.eliteprospects.com/league/' + league + '/stats/' + year + '?page-goalie='\n",
    "    # print('Collects data from ' + 'https://www.eliteprospects.com/league/' + league + '/stats/' + year)\n",
    "    \n",
    "    print(\"Beginning scrape of \" + league + \" goalie data from \" + year + \".\")\n",
    "    \n",
    "    # Return list with all plyers for season in link     \n",
    "    players = []\n",
    "    \n",
    "    page = (requests.get(url + str(1) + \"#goalies\", timeout = 500))\n",
    "    first_page_string = str(page)\n",
    "    \n",
    "    while first_page_string == '<Response [403]>':\n",
    "        print(\"Just got a 403 Error before entering the page. This means EliteProspects has temporarily blocked your IP address.\")\n",
    "        print(\"We're going to sleep for 60 seconds, then try again.\")\n",
    "        time.sleep(100)\n",
    "        page = (requests.get(url + str(1) + \"#goalies\", timeout = 500))\n",
    "        first_page_string = str(page)\n",
    "        print(\"Okay, let's try this again\")\n",
    "    \n",
    "    if (first_page_string) == '<Response [404]>':\n",
    "        print(\"ERROR: \" + first_page_string + \" on league: \" + league + \" in year: \" + year + \". Data doesn't exist for this league and season.\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(1,99):\n",
    "            page = requests.get(url+str(i), timeout = 500)\n",
    "            page_string = str(page)\n",
    "            \n",
    "            while page_string == '<Response [403]>':\n",
    "                print(\"Just got a 403 Error within the page. Time to Sleep, then re-obtain the link.\")\n",
    "                time.sleep(100)\n",
    "                page = (requests.get(url+str(i), timeout = 500))\n",
    "                page_string = str(page)\n",
    "                print(\"Changed the string within the page. Let's try again\")\n",
    "                \n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "            # Get data for players table\n",
    "            player_table = soup.find(\"table\", {\"class\":\"table table-striped table-sortable goalie-stats highlight-stats season\"})\n",
    "\n",
    "            try:\n",
    "                df_players = tableDataText(player_table)\n",
    "            except AttributeError:\n",
    "                print(\"BREAK: TABLE NONE ERROR: \" + str(requests.get(url+str(i)), timeout = 500) + \" On League: \" + league + \" In Year: \" + year)\n",
    "                break\n",
    "                \n",
    "            if len(df_players)>0:\n",
    "\n",
    "                if df_players['#'].count()>0:\n",
    "                    # Remove empty rows\n",
    "                    df_players = df_players[df_players['#']!=''].reset_index(drop=True)\n",
    "\n",
    "                    # Extract href links in table\n",
    "                    href_row = []\n",
    "                    for link in player_table.find_all('a'):\n",
    "                        href_row.append(link.attrs['href'])\n",
    "\n",
    "                    # Create data frame, rename and only keep links to players\n",
    "                    df_links = pd.DataFrame(href_row)  \n",
    "                    df_links.rename(columns={ df_links.columns[0]:\"link\"}, inplace=True)\n",
    "                    df_links= df_links[df_links['link'].str.contains(\"/player/\")].reset_index(drop=True)    \n",
    "\n",
    "                    # Add links to players\n",
    "                    df_players['link']=df_links['link'] \n",
    "\n",
    "                    players.append(df_players)\n",
    "\n",
    "                    # Wait 3 seconds before going to next\n",
    "                    # time.sleep(1)\n",
    "                    #print(\"Scraped page \" + str(i))\n",
    "                    \n",
    "            else:\n",
    "                #print(\"Scraped final page of: \" + league + \" In Year: \" + year)\n",
    "                break\n",
    "\n",
    "    \n",
    "        if len(players)!=0:\n",
    "            df_players = pd.concat(players).reset_index()\n",
    "\n",
    "            df_players.columns = map(str.lower, df_players.columns)\n",
    "\n",
    "            # Clean up dataset\n",
    "            df_players['season'] = year\n",
    "            df_players['league'] = league\n",
    "\n",
    "            df_players = df_players.drop(['index','#'], axis=1).reset_index(drop=True)\n",
    "            \n",
    "            print(\"Successfully scraped all \" + league + \" goalie data from \" + year + \".\")\n",
    "            \n",
    "            df_players = df_players.loc[((df_players.gp!=0) & (~pd.isna(df_players.gp)) & (df_players.gp!=\"0\") & (df_players.gaa!=\"-\"))]\n",
    "\n",
    "            return df_players\n",
    "        else: print(\"LENGTH 0 ERROR: \" + str(requests.get(url+str(1), timeout = 500)) + \" On League: \" + league + \" In Year: \" + year)  \n",
    "    \n",
    "def get_info(link):\n",
    "    \n",
    "    page = requests.get(link, timeout = 500)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    page_string = str(page)\n",
    "\n",
    "    while ((page_string == '<Response [403]>') or (\"evil\" in str(soup.p))): \n",
    "        print(\"403 Error. re-obtaining string and re-trying.\")\n",
    "        page = requests.get(link, timeout = 500)\n",
    "        page_string = str(page)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        time.sleep(60)\n",
    "\n",
    "    if soup.find(\"title\") != None:\n",
    "        player = soup.find(\"title\").string.replace(\" - Elite Prospects\" ,\"\")\n",
    "\n",
    "    else: player = \"-\"\n",
    "\n",
    "    if (soup.find(\"div\", {\"class\":\"col-xs-12 col-17 text-right p-0 ep-text-color--black\"}))!= None:\n",
    "        if 'dob' in (soup.find(\"div\", {\"class\":\"col-xs-12 col-17 text-right p-0 ep-text-color--black\"})).find(\"a\")['href']:\n",
    "            dob = soup.find(\"div\", {\"class\":\"col-xs-12 col-17 text-right p-0 ep-text-color--black\"}).find(\"a\")['href'].split(\"dob=\", 1)[1].split(\"&sort\", 1)[0]\n",
    "        else: \n",
    "            dob = \"-\"\n",
    "\n",
    "    else:\n",
    "        dob = \"-\"\n",
    "\n",
    "    if soup.find(\"div\", {\"class\":\"order-6 order-sm-3 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}) != None:\n",
    "        if \"cm\" in soup.find(\"div\", {\"class\":\"order-6 order-sm-3 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                            ).find(\n",
    "        \"div\", {\"class\":\"col-xs-12 col-18 text-right p-0 ep-text-color--black\"}).string:\n",
    "            height = soup.find(\"div\", {\"class\":\"order-6 order-sm-3 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                              ).find(\n",
    "            \"div\", {\"class\":\"col-xs-12 col-18 text-right p-0 ep-text-color--black\"}).string.split(\" / \")[1].split(\"cm\")[0].strip()\n",
    "        else: \n",
    "            height = \"-\"\n",
    "\n",
    "    else: \n",
    "        height = \"-\"\n",
    "\n",
    "    if soup.find(\"div\", {\"class\":\"order-7 order-sm-5 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}) != None:\n",
    "        if soup.find(\"div\", {\"class\":\"order-7 order-sm-5 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                    ).find(\n",
    "            \"div\", {\"class\":\"col-xs-12 col-18 text-right p-0 ep-text-color--black\"}).string.split(\"\\n\")[1].split(\"lbs\")[0].strip() == '- / -':\n",
    "                weight = \"-\"\n",
    "        else: \n",
    "            weight = soup.find(\"div\", {\"class\":\"order-7 order-sm-5 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                              ).find(\n",
    "            \"div\", {\"class\":\"col-xs-12 col-18 text-right p-0 ep-text-color--black\"}).string.split(\"\\n\")[1].split(\"lbs\")[0].strip()\n",
    "\n",
    "    else: weight = \"-\"\n",
    "\n",
    "    if soup.find(\"div\", {\"class\":\"order-2 order-sm-4 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                         ) != None:\n",
    "        if soup.find(\"div\", {\"class\":\"order-2 order-sm-4 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                         ).find(\n",
    "            \"div\", {\"class\":\"col-xs-12 col-17 text-right p-0 ep-text-color--black\"}).find(\"a\") != None:\n",
    "\n",
    "            birthplace = soup.find(\"div\", {\"class\":\"order-2 order-sm-4 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                         ).find(\n",
    "                        \"div\", {\"class\":\"col-xs-12 col-17 text-right p-0 ep-text-color--black\"}).find(\"a\").string.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "        else: \n",
    "            birthplace = \"-\"\n",
    "    else: \n",
    "        birthplace = \"-\"\n",
    "\n",
    "    if soup.find(\"div\", {\"class\":\"order-3 order-sm-6 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}) != None:\n",
    "        if soup.find(\"div\", {\"class\":\"order-3 order-sm-6 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                    ).find(\n",
    "            \"div\", {\"class\":\"col-xs-12 col-18 text-right p-0 ep-text-color--black\"}).find(\"a\") != None:\n",
    "                nation = soup.find(\"div\", {\"class\":\"order-3 order-sm-6 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                                  ).find(\n",
    "                \"div\", {\"class\":\"col-xs-12 col-18 text-right p-0 ep-text-color--black\"}).find(\"a\").string.replace(\"\\n\", \"\").strip()\n",
    "        else: nation = \"-\"\n",
    "\n",
    "    else:\n",
    "        nation = \"-\"\n",
    "\n",
    "    if soup.find(\"div\", {\"class\":\"order-8 order-sm-7 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}) !=None:\n",
    "        shoots = soup.find(\"div\", {\"class\":\"order-8 order-sm-7 ep-list__item ep-list__item--col-2 ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                          ).find(\n",
    "            \"div\", {\"class\":\"col-xs-12 col-18 text-right p-0 ep-text-color--black\"}).string.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "    else:\n",
    "        shoots = \"-\"\n",
    "\n",
    "    if soup.find(\"div\", {\"class\":\"order-12 ep-list__item ep-list__item--in-card-body ep-list__item--is-compact\"}) != None:\n",
    "        draft = soup.find(\"div\", {\"class\":\"order-12 ep-list__item ep-list__item--in-card-body ep-list__item--is-compact\"}\n",
    "                         ).find(\n",
    "            \"div\", {\"class\":\"col-xs-12 col-18 text-right p-0\"}).find(\"a\").string.replace(\"\\n\", \"\").strip()\n",
    "    else: \n",
    "        draft = \"-\"\n",
    "\n",
    "    #height = np.where(height==\"- / -\", \"-\", height)\n",
    "\n",
    "    #print(player + \" scraped!\")\n",
    "    return(player, dob, height, weight, birthplace, nation, shoots, draft, link)\n",
    "    \n",
    "def get_player_information(dataframe):\n",
    "\n",
    "    myplayer = []\n",
    "    mydob = []\n",
    "    myheight = []\n",
    "    myweight = []\n",
    "    mybirthplace = []\n",
    "    mynation = []\n",
    "    myshot = []\n",
    "    mydraft = []\n",
    "    mylink = []\n",
    "    \n",
    "    print(\"Beginning scrape for \" + str(len(list(set(dataframe.link)))) + \" players.\")\n",
    "\n",
    "    for i in range(0, len(list(set(dataframe.link)))):\n",
    "        try:\n",
    "            myresult = get_info(((list(set(dataframe.link))[i])))\n",
    "            myplayer.append(myresult[0])\n",
    "            mydob.append(myresult[1])\n",
    "            myheight.append(myresult[2])\n",
    "            myweight.append(myresult[3])\n",
    "            mybirthplace.append(myresult[4])\n",
    "            mynation.append(myresult[5])\n",
    "            myshot.append(myresult[6])\n",
    "            mydraft.append(myresult[7])\n",
    "            mylink.append(myresult[8])\n",
    "            print(myresult[0] + \" scraped! That's \" + str(i + 1) + \" down! Only \" + str(len(list(set(dataframe.link))) - (i + 1)) +  \" left to go!\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"You interrupted this one manually. The output here will be every player you've scraped so far. Good bye!\")\n",
    "            break\n",
    "        except (ConnectionError,\n",
    "            HTTPError,\n",
    "            ReadTimeout,\n",
    "            ConnectTimeout) as errormessage:\n",
    "            print(\"Yo' ass is disconnected! Here's the error message:\")\n",
    "            print(errormessage)\n",
    "            print(\"Luckily, everything you've scraped up to this point will still be safe.\")\n",
    "            break\n",
    "\n",
    "    resultdf = pd.DataFrame(columns = [\"player\", \"dob\", \"height\", \"weight\", \"birthplace\", \"nation\", \"shoots\", \"draft\", \"link\"])\n",
    "\n",
    "    resultdf.player = myplayer\n",
    "    resultdf.dob = mydob\n",
    "    resultdf.height = myheight\n",
    "    resultdf.weight = myweight\n",
    "    resultdf.birthplace = mybirthplace\n",
    "    resultdf.nation = mynation\n",
    "    resultdf.shoots = myshot\n",
    "    resultdf.draft = mydraft\n",
    "    resultdf.link = mylink\n",
    "    \n",
    "    print(\"Your scrape is complete! You've obtained player information for \" + str(len(resultdf)) + \" players!\")\n",
    "    \n",
    "    return resultdf\n",
    "\n",
    "\n",
    "        \n",
    "def get_league_skater_boxcars(league, seasons):\n",
    "\n",
    "    if len(set(seasons))==1:\n",
    "        scraped_season_list = str(seasons)\n",
    "    elif len(set(seasons))>2:\n",
    "        scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "    else:\n",
    "        scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "    \n",
    "    \n",
    "    global hidden_patrick\n",
    "    hidden_patrick = 0\n",
    "    global error\n",
    "    error = 0\n",
    "    \n",
    "    output = pd.DataFrame()\n",
    "    \n",
    "    if type(seasons) == str:\n",
    "        single = getskaters(league, seasons)\n",
    "        output = output.append(single)\n",
    "        print(\"Scraping \" + league + \" data is complete. You scraped skater data from \" + seasons + \".\")\n",
    "        return(output)\n",
    "    \n",
    "    elif ((type(seasons) == tuple) or (type(seasons) == list)):\n",
    "    \n",
    "        for i in range(0, len(seasons)):\n",
    "            try:\n",
    "                single = getskaters(league, seasons[i])\n",
    "                output = output.append(single)\n",
    "            except KeyboardInterrupt as e:\n",
    "                hidden_patrick = 4\n",
    "                error = e\n",
    "                return output\n",
    "            except (ConnectionError,\n",
    "                HTTPError,\n",
    "                ReadTimeout,\n",
    "                ConnectTimeout) as e:\n",
    "                hidden_patrick = 5\n",
    "                error = e\n",
    "                return output\n",
    "            \n",
    "        print(\"Scraping \" + league + \" data is complete. You scraped skater data from \" + scraped_season_list + \".\")    \n",
    "        return(output)\n",
    "    \n",
    "def get_league_goalie_boxcars(league, seasons):\n",
    "\n",
    "    if len(set(seasons))==1:\n",
    "        scraped_season_list = str(seasons)\n",
    "    elif len(set(seasons))>2:\n",
    "        scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "    else:\n",
    "        scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "    \n",
    "    \n",
    "    global hidden_patrick\n",
    "    hidden_patrick = 0\n",
    "    global error\n",
    "    error = 0\n",
    "    \n",
    "    output = pd.DataFrame()\n",
    "    \n",
    "    if type(seasons) == str:\n",
    "        single = getgoalies(league, seasons)\n",
    "        output = output.append(single)\n",
    "        print(\"Scraping \" + league + \" data is complete. You scraped goalie data from \" + seasons + \".\")\n",
    "        return(output)\n",
    "    \n",
    "    elif ((type(seasons) == tuple) or (type(seasons) == list)):\n",
    "    \n",
    "        for i in range(0, len(seasons)):\n",
    "            try:\n",
    "                single = getgoalies(league, seasons[i])\n",
    "                output = output.append(single)\n",
    "            except KeyboardInterrupt as e:\n",
    "                hidden_patrick = 4\n",
    "                error = e\n",
    "                return output\n",
    "            except (ConnectionError,\n",
    "                HTTPError,\n",
    "                ReadTimeout,\n",
    "                ConnectTimeout) as e:\n",
    "                hidden_patrick = 5\n",
    "                error = e\n",
    "                return output\n",
    "            \n",
    "        print(\"Scraping \" + league + \" data is complete. You scraped goalie data from \" + scraped_season_list + \".\")    \n",
    "        return(output)\n",
    "\n",
    "def get_goalies(leagues, seasons):\n",
    "    \n",
    "    if (len(seasons)==1 or type(seasons)==str):\n",
    "        season_string = str(seasons)\n",
    "    elif len(seasons)==2:\n",
    "        season_string = \" and\".join(str((tuple(sorted(tuple(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").split(\",\"))\n",
    "    else:\n",
    "        season_string = str(((tuple(sorted(tuple(seasons)))))[:-1]).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\") + \" and \" + str(((tuple(sorted(tuple(seasons)))))[-1])\n",
    "        \n",
    "    if (len(leagues)==1 or type(leagues)==str):\n",
    "        league_string = str(leagues)\n",
    "    elif len(leagues)==2:\n",
    "        league_string = \" and\".join(str((tuple(sorted(tuple(leagues))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").split(\",\"))\n",
    "    else:\n",
    "        league_string = str(((tuple(sorted(tuple(leagues)))))[:-1]).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\") + \" and \" + str(((tuple(sorted(tuple(leagues)))))[-1])\n",
    "    \n",
    "    leaguesall = pd.DataFrame()\n",
    "\n",
    "    if ((type(leagues)==str) and (type(seasons)==str)):\n",
    "        print(\"Your scrape request is goalie data from the following league:\")\n",
    "        print(league_string)\n",
    "        print(\"In the following season:\")\n",
    "        print(season_string)\n",
    "        leaguesall = get_league_goalie_boxcars(leagues, seasons)\n",
    "        print(\"Completed scraping goalie data from the following league:\")\n",
    "        print(str(leagues))\n",
    "        print(\"Over the following season:\")\n",
    "        print(str(seasons))\n",
    "        return(leaguesall.reset_index().drop(columns = 'index')) \n",
    "        \n",
    "    elif ((type(leagues)==str) and ((type(seasons) == tuple) or (type(seasons) == list))):\n",
    "        print(\"Your scrape request is goalie data from the following league:\")\n",
    "        print(league_string)\n",
    "        print(\"In the following seasons:\")\n",
    "        print(season_string)\n",
    "        leaguesall = get_league_goalie_boxcars(leagues, seasons)\n",
    "        \n",
    "        if hidden_patrick == 4:\n",
    "            print(\"You interrupted this one manually. The output here will be every player you've scraped so far. Good bye!\")\n",
    "            return(leaguesall.reset_index().drop(columns = 'index')) \n",
    "        if hidden_patrick == 5:\n",
    "            print(\"You were disconnected! The output here will be every player you've scraped so far. Here's your error message:\")\n",
    "            print(error)\n",
    "            return(leaguesall.reset_index().drop(columns = 'index')) \n",
    "        \n",
    "        if len(set(leaguesall.league))==1:\n",
    "            scraped_league_list = leaguesall.league\n",
    "        elif len(set(leaguesall.league))>2:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        if len(set(seasons))==1:\n",
    "            scraped_season_list = seasons\n",
    "        elif len(set(seasons))>2:\n",
    "            scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        print(\"Completed scraping goalie data from the following league:\")\n",
    "        print(str(leagues))\n",
    "        print(\"Over the following seasons:\")\n",
    "        print(scraped_season_list)\n",
    "        return(leaguesall.reset_index().drop(columns = 'index'))    \n",
    "    \n",
    "    elif ((type(seasons) == str) and ((type(leagues) == tuple) or (type(leagues) == list))):\n",
    "        print(\"Your scrape request is goalie data from the following leagues:\")\n",
    "        print(league_string)\n",
    "        print(\"In the following season:\")\n",
    "        print(season_string)\n",
    "        \n",
    "        for i in range (0, len(leagues)):\n",
    "            try:\n",
    "                targetleague = get_league_goalie_boxcars(leagues[i], seasons)\n",
    "                leaguesall = leaguesall.append(targetleague)\n",
    "                if hidden_patrick == 4:\n",
    "                    raise KeyboardInterrupt\n",
    "                if hidden_patrick == 5:\n",
    "                    raise Patrick\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"You interrupted this one manually. The output here will be every player you've scraped so far. Good bye!\")\n",
    "                break\n",
    "            except Patrick:\n",
    "                print(\"You were disconnected! The output here will be every player you've scraped so far. Here's your error message:\")\n",
    "                print(error)\n",
    "                break\n",
    "                \n",
    "        if len(set(leaguesall.league))==1:\n",
    "            scraped_league_list = leaguesall.league\n",
    "        elif len(set(leaguesall.league))>2:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        print(\"Completed scraping goalie data from the following leagues:\")\n",
    "        print(scraped_league_list)\n",
    "        print(\"Over the following season:\")\n",
    "        print((seasons))\n",
    "        return(leaguesall.reset_index().drop(columns = 'index'))    \n",
    "            \n",
    "    elif (((type(seasons) == tuple) or (type(seasons) == list)) and ((type(leagues) == tuple) or (type(leagues) == list))):\n",
    "        print(\"Your scrape request is goalie data from the following leagues:\")\n",
    "        print(league_string)\n",
    "        print(\"In the following seasons:\")\n",
    "        print(season_string)\n",
    "        #print(\"Your scrape request: \" + str(leagues[:-1]).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\") + \", and \" + (leagues)[-1] + \" goalie data from \" +str(seasons[:-1]).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\") + \", and \" + (seasons)[-1] + \".\")    \n",
    "        for i in range (0, len(leagues)):\n",
    "            try:\n",
    "                targetleague = get_league_goalie_boxcars(leagues[i], seasons)\n",
    "                leaguesall = leaguesall.append(targetleague)\n",
    "                if hidden_patrick == 4:\n",
    "                    raise KeyboardInterrupt\n",
    "                if hidden_patrick == 5:\n",
    "                    raise Patrick\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"You interrupted this one manually. The output here will be every player you've scraped so far. Good bye!\")\n",
    "                break\n",
    "            except Patrick:\n",
    "                print(\"You were disconnected! The output here will be every player you've scraped so far. Here's your error message:\")\n",
    "                print(error)\n",
    "                break\n",
    "                \n",
    "        if len(set(leaguesall.league))==1:\n",
    "            scraped_league_list = leaguesall.league\n",
    "        elif len(set(leaguesall.league))>2:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        if len(set(seasons))==1:\n",
    "            scraped_season_list = seasons\n",
    "        elif len(set(seasons))>2:\n",
    "            scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        print(\"Completed scraping goalie data from the following leagues:\")\n",
    "        print(scraped_league_list)\n",
    "        print(\"Over the following seasons:\")\n",
    "        print(scraped_season_list)\n",
    "        return(leaguesall.reset_index().drop(columns = 'index'))        \n",
    "                \n",
    "    else:\n",
    "        print(\"There was an issue with the request you made. Please enter a single league and season as a string, or multiple leagues as either a list or tuple.\")\n",
    "    \n",
    "    \n",
    "def get_skaters(leagues, seasons):\n",
    "    \n",
    "    if (len(seasons)==1 or type(seasons)==str):\n",
    "        season_string = str(seasons)\n",
    "    elif len(seasons)==2:\n",
    "        season_string = \" and\".join(str((tuple(sorted(tuple(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").split(\",\"))\n",
    "    else:\n",
    "        season_string = str(((tuple(sorted(tuple(seasons)))))[:-1]).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\") + \" and \" + str(((tuple(sorted(tuple(seasons)))))[-1])\n",
    "        \n",
    "    if (len(leagues)==1 or type(leagues)==str):\n",
    "        league_string = str(leagues)\n",
    "    elif len(leagues)==2:\n",
    "        league_string = \" and\".join(str((tuple(sorted(tuple(leagues))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").split(\",\"))\n",
    "    else:\n",
    "        league_string = str(((tuple(sorted(tuple(leagues)))))[:-1]).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\") + \" and \" + str(((tuple(sorted(tuple(leagues)))))[-1])\n",
    "    \n",
    "    leaguesall = pd.DataFrame()\n",
    "\n",
    "    if ((type(leagues)==str) and (type(seasons)==str)):\n",
    "        print(\"Your scrape request is skater data from the following league:\")\n",
    "        print(league_string)\n",
    "        print(\"In the following season:\")\n",
    "        print(season_string)\n",
    "        leaguesall = get_league_skater_boxcars(leagues, seasons)\n",
    "        print(\"Completed scraping skater data from the following league:\")\n",
    "        print(str(leagues))\n",
    "        print(\"Over the following season:\")\n",
    "        print(str(seasons))\n",
    "        return(leaguesall.reset_index().drop(columns = 'index')) \n",
    "        \n",
    "    elif ((type(leagues)==str) and ((type(seasons) == tuple) or (type(seasons) == list))):\n",
    "        print(\"Your scrape request is skater data from the following league:\")\n",
    "        print(league_string)\n",
    "        print(\"In the following seasons:\")\n",
    "        print(season_string)\n",
    "        leaguesall = get_league_skater_boxcars(leagues, seasons)\n",
    "        \n",
    "        if hidden_patrick == 4:\n",
    "            print(\"You interrupted this one manually. The output here will be every player you've scraped so far. Good bye!\")\n",
    "            return(leaguesall.reset_index().drop(columns = 'index')) \n",
    "        if hidden_patrick == 5:\n",
    "            print(\"You were disconnected! The output here will be every player you've scraped so far. Here's your error message:\")\n",
    "            print(error)\n",
    "            return(leaguesall.reset_index().drop(columns = 'index')) \n",
    "        \n",
    "        if len(set(leaguesall.league))==1:\n",
    "            scraped_league_list = leaguesall.league\n",
    "        elif len(set(leaguesall.league))>2:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        if len(set(seasons))==1:\n",
    "            scraped_season_list = seasons\n",
    "        elif len(set(seasons))>2:\n",
    "            scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        print(\"Completed scraping skater data from the following league:\")\n",
    "        print(str(leagues))\n",
    "        print(\"Over the following seasons:\")\n",
    "        print(scraped_season_list)\n",
    "        return(leaguesall.reset_index().drop(columns = 'index'))    \n",
    "    \n",
    "    elif ((type(seasons) == str) and ((type(leagues) == tuple) or (type(leagues) == list))):\n",
    "        print(\"Your scrape request is skater data from the following leagues:\")\n",
    "        print(league_string)\n",
    "        print(\"In the following season:\")\n",
    "        print(season_string)\n",
    "        \n",
    "        for i in range (0, len(leagues)):\n",
    "            try:\n",
    "                targetleague = get_league_skater_boxcars(leagues[i], seasons)\n",
    "                leaguesall = leaguesall.append(targetleague)\n",
    "                if hidden_patrick == 4:\n",
    "                    raise KeyboardInterrupt\n",
    "                if hidden_patrick == 5:\n",
    "                    raise Patrick\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"You interrupted this one manually. The output here will be every player you've scraped so far. Good bye!\")\n",
    "                break\n",
    "            except Patrick:\n",
    "                print(\"You were disconnected! The output here will be every player you've scraped so far. Here's your error message:\")\n",
    "                print(error)\n",
    "                break\n",
    "                \n",
    "        if len(set(leaguesall.league))==1:\n",
    "            scraped_league_list = leaguesall.league\n",
    "        elif len(set(leaguesall.league))>2:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        print(\"Completed scraping skater data from the following leagues:\")\n",
    "        print(scraped_league_list)\n",
    "        print(\"Over the following season:\")\n",
    "        print((seasons))\n",
    "        return(leaguesall.reset_index().drop(columns = 'index'))    \n",
    "            \n",
    "    elif (((type(seasons) == tuple) or (type(seasons) == list)) and ((type(leagues) == tuple) or (type(leagues) == list))):\n",
    "        print(\"Your scrape request is skater data from the following leagues:\")\n",
    "        print(league_string)\n",
    "        print(\"In the following seasons:\")\n",
    "        print(season_string)\n",
    "        #print(\"Your scrape request: \" + str(leagues[:-1]).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\") + \", and \" + (leagues)[-1] + \" skater data from \" +str(seasons[:-1]).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\") + \", and \" + (seasons)[-1] + \".\")    \n",
    "        for i in range (0, len(leagues)):\n",
    "            try:\n",
    "                targetleague = get_league_skater_boxcars(leagues[i], seasons)\n",
    "                leaguesall = leaguesall.append(targetleague)\n",
    "                if hidden_patrick == 4:\n",
    "                    raise KeyboardInterrupt\n",
    "                if hidden_patrick == 5:\n",
    "                    raise Patrick\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"You interrupted this one manually. The output here will be every player you've scraped so far. Good bye!\")\n",
    "                break\n",
    "            except Patrick:\n",
    "                print(\"You were disconnected! The output here will be every player you've scraped so far. Here's your error message:\")\n",
    "                print(error)\n",
    "                break\n",
    "                \n",
    "        if len(set(leaguesall.league))==1:\n",
    "            scraped_league_list = leaguesall.league\n",
    "        elif len(set(leaguesall.league))>2:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_league_list = str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(list(set(leaguesall.league))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        if len(set(seasons))==1:\n",
    "            scraped_season_list = seasons\n",
    "        elif len(set(seasons))>2:\n",
    "            scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \", and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        else:\n",
    "            scraped_season_list = str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[:-1]).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \" and \" + str(((str(tuple(sorted(tuple(set(seasons))))).replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))).split(\", \")[-1])\n",
    "        \n",
    "        print(\"Completed scraping skater data from the following leagues:\")\n",
    "        print(scraped_league_list)\n",
    "        print(\"Incorporating the following seasons:\")\n",
    "        print(scraped_season_list)\n",
    "        return(leaguesall.reset_index().drop(columns = 'index'))        \n",
    "                \n",
    "    else:\n",
    "        print(\"There was an issue with the request you made. Please enter a single league and season as a string, or multiple leagues as either a list or tuple.\")\n",
    "\n",
    "def add_player_information(dataframe):\n",
    "    with_player_info = get_player_information(dataframe)\n",
    "    doubledup = dataframe.merge(with_player_info.drop(columns = ['player']), on = 'link', how = 'inner')\n",
    "    return doubledup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE ONE: GET ALL SKATERS FROM THE MHL IN 2020-2021 ###\n",
    "\n",
    "mhl2021 = get_skaters(\"mhl\", \"2020-2021\")\n",
    "\n",
    "### NOW SORT THESE SKATERS BY POINTS PER GAME (\"PPG\") AND TAKE A LOOK AT THE TOP 20 ###\n",
    "\n",
    "mhl2021.sort_values(by = 'ppg', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE 2: GET DATA FOR MULTIPLE LEAGUES AND SEASONS ###\n",
    "\n",
    "leagues = (\"mhl\", \"khl\")\n",
    "\n",
    "seasons = (\"2019-2020\", \"2020-2021\")\n",
    "\n",
    "mhl_khl_1921 = get_skaters(leagues, seasons)\n",
    "\n",
    "### DISPLAY DATA FROM ONLY ONE OF THOSE LEAGUES USING LOC ###\n",
    "\n",
    "mhl_khl_1921.loc[mhl_khl_1921.league==\"khl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-organ",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### EXAMPLE 3: ADD PLAYER INFORMATION DATA FOR THE TOP-20 MHL SCORERS WE LOOKED AT BEFORE, THEN TAKE A LOOK AT IT. ###\n",
    "\n",
    "mhl_top_20 = mhl2021.sort_values(by = 'ppg', ascending = False).head(20)\n",
    "\n",
    "mhl_top_20_with_info = add_player_information(mhl_top_20)\n",
    "\n",
    "mhl_top_20_with_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-strength",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### EXAMPLE 4: LOOK AT SOME PRO GOALIES PLAYING IN AMERICA OVER THE PAST 3 YEARS ###\n",
    "\n",
    "usaproleagues = (\"ahl\", \"nhl\")\n",
    "last3years = (\"2017-2018\", \"2018-2019\", \"2019-2020\")\n",
    "\n",
    "usaprogoalies = get_goalies(usaproleagues, last3years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
